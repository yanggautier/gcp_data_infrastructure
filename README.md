# README - Data Pipeline avec Apache Beam, Dataflow et Cloud Composer

## ğŸ“Œ Description du projet
Ce projet implÃ©mente une architecture de pipeline de donnÃ©es sur Google Cloud Platform (GCP) pour traiter et charger des fichiers CSV dans BigQuery. Il utilise **Apache Beam**, **Dataflow**, et **Cloud Composer (Airflow)** pour orchestrer le traitement et le chargement des donnÃ©es.

## ğŸ“‚ Arborescence du projet
```
.
â”œâ”€â”€ Data dictionary.xlsx          # Dictionnaire des donnÃ©es
â”œâ”€â”€ README.md                     # Documentation du projet
â”œâ”€â”€ cloud_composer                # Contient les DAGs pour orchestrer les pipelines Dataflow
â”‚   â””â”€â”€ dags
â”‚       â”œâ”€â”€ composer_dataflow_dag.py  # DAG pour exÃ©cuter le pipeline GCS â†’ BigQuery
â”‚       â””â”€â”€ gsc_to_bigquery_dag.py    # DAG supplÃ©mentaire pour ingestion
â”œâ”€â”€ dataflow                       # Contient les pipelines Apache Beam
â”‚   â”œâ”€â”€ gcs_to_bigquery_pipeline.py  # Pipeline Beam pour charger les donnÃ©es dans BigQuery
â”‚   â””â”€â”€ gcs_to_gcs_pipeline.py       # Pipeline Beam pour transformer et stocker les donnÃ©es dans GCS
â”œâ”€â”€ notebooks                      # Analyses exploratoires et tests
â”œâ”€â”€ sqls                           # Scripts SQL pour BigQuery et Cloud SQL
â””â”€â”€ requirements.txt               # DÃ©pendances du projet
```

## ğŸš€ Technologies utilisÃ©es
- **Google Cloud Platform (GCP)** : Cloud Storage, Dataflow, BigQuery, Cloud Composer
- **Apache Beam** : Framework pour traitement de flux de donnÃ©es
- **Google Cloud Dataflow** : ExÃ©cution des pipelines Apache Beam
- **Apache Airflow (Cloud Composer)** : Orchestration des tÃ¢ches
- **Python** : DÃ©veloppement des pipelines

## ğŸ”„ Fonctionnement des pipelines Apache Beam
### 1ï¸âƒ£ **Pipeline GCS â†’ GCS (Transformation des CSV) **
Fichier concernÃ© : `gcs_to_gcs_pipeline.py`
- Lit les fichiers CSV depuis un bucket GCS
- Applique des transformations (conversion des dates, suppression des doublons)
- Sauvegarde les fichiers transformÃ©s dans un autre bucket

### 2ï¸âƒ£ **Pipeline GCS â†’ BigQuery**
Fichier concernÃ© : `gcs_to_bigquery_pipeline.py`
- Lit les fichiers CSV depuis GCS
- Nettoie et transforme les donnÃ©es en fonction dâ€™un schÃ©ma
- Charge les donnÃ©es dans des tables BigQuery

### 3ï¸âƒ£ **Orchestration avec Airflow (Cloud Composer)**
DAG concernÃ© : `composer_dataflow_dag.py`
- DÃ©clenche le pipeline Dataflow `gcs_to_bigquery_pipeline.py`
- Utilise `DataflowCreatePythonJobOperator` pour soumettre le job
- ExÃ©cute le pipeline pour chaque table dÃ©finie dans BigQuery

## ğŸ“Œ Instructions d'exÃ©cution
### 1ï¸âƒ£ **DÃ©ploiement du pipeline Apache Beam sur Dataflow**
```sh
python dataflow/gcs_to_gcs_pipeline.py \
  --runner=DataflowRunner \
  --project=your-gcp-project \
  --region=europe-west9 \
  --staging_location=gs://your-bucket/staging/ \
  --temp_location=gs://your-bucket/temp/ \
  --input_bucket=gs://your-input-bucket \
  --output_bucket=gs://your-output-bucket
```

### 2ï¸âƒ£ **ExÃ©cution du DAG Airflow dans Cloud Composer**
1. Se connecter Ã  lâ€™interface Airflow de Cloud Composer
2. Activer et exÃ©cuter le DAG `composer_dataflow_dag`
3. VÃ©rifier lâ€™Ã©tat des tÃ¢ches et les logs dans Airflow


## ğŸ“Œ AmÃ©liorations possibles
- Automatiser la dÃ©tection des schÃ©mas pour BigQuery
- Ajouter des tests unitaires pour les transformations de donnÃ©es
- Optimiser le pipeline pour supporter des volumes de donnÃ©es plus importants

---

ğŸ“¢ **Auteur** : Projet conÃ§u pour orchestrer et transformer des donnÃ©es e-commerce avec GCP ğŸš€

